---
layout: post
title: "영상의 특징값 추출 방법"
description: "마스크 기반 엣지 검출 / 캐니 엣지 검출기 / 허프 변환을 이용한 직선 검출 / 해리스 코너 포인트 검출"
tags: [AI, 영상처리]
---

**특징값**(feature)이란?

- 영상 내에서 다른 부분과 구분되어 두드러지는 성질
- 특정 사물의 위치를 찾는 데 사용
- 영상 간의 유사성을 판단하는 기준으로 사용

특징값 종류

- 점 (point)
- 엣지 (edge) / 경계선 / 외곽선
- 코너 (corner) / 모서리
- 질감 (texture)
- 색상 (color)

### **엣지**

**영상에서 객체의 외곽선처럼 배경과 객체의 경계가 되는 부분**을 가리킨다. 일반적으로 객체와 배경의 경계에서는 픽셀의 밝기 값이 크게 변하게 되므로 그 값의 변화가 큰 부분을 엣지로 판별할 수 있다.

![image1](https://github.com/ufolozie/ufolozie.github.io/assets/98166928/e737f8d9-554a-4da6-84ae-61520d2a36db)

영상에서 엣지를 검출하는 대표적인 방법은 영상의 1차 미분 값을 이용하는 것이다. **미분**이란 **함수의 변화량**을 의미하며, 함수 f(x, y)의 x축 방향으로의 미분은 다음과 같은 수식으로 표현할 수 있다.

$$
\frac{\partial}{\partial x}f,\frac{\partial f}{\partial x}, f_x 
$$

일반적으로 영상의 엣지에서는 그레이스케일 값이 급격하게 변화하기 때문에 영상의 미분 값 또한 크게 나타난다. 그러므로 영상의 미분 함수를 구하고, 그 값이 크게 나타나는 위치를 엣지 위치로 간주할 수 있다.

![image2](https://github.com/ufolozie/ufolozie.github.io/assets/98166928/96caef18-54d9-4ed1-a2c8-1c9a2b5d2d97){: width="90%" style="display: block; margin: auto;"}

위 그림은 연속 함수 f(x, y)에 대하여 엣지에서의 미분 값을 보여준다. y 값은 고정되어 있고, x 값에 변화에 따른 f(x, y)의 변화만을 나타낸다.

왼쪽 그림은 입력 함수의 모습을 보여주며, x = t 부근에서 값이 급격하게 증가하는 것을 볼 수 있다. 이러한 함숫값의 변화는 영상의 엣지 부근에서 나타나는 현상이며, 실제 영상에서는 x 값이 증가함에 따라 밝기가 급격하게 증가하는 현상으로 나타난다.

오른쪽 그림은 함수 f(x, y)의 x축 방향으로의 1차 미분을 그래프로 나타낸 것이다. 함수 f<sub>x</sub>(x, y)의 값은 x = t 부근에서만 큰 값을 갖고, 나머지 부분에서는 거의 0 값을 갖는다. 그러므로 영상에서 엣지를 검출하기 위해서는 영상을 미분한 후, 미분 값이 특정 임계값(threshold)보다 큰 부분을 찾으면 된다.  
**임계값: 하나의 변수 x가 어느 값이 되었을 때 특이한 상태나 급격한 변화가 일어나 임계 상태에 있을 때의 x값. 임계란 ‘경계’와 비슷한 개념으로, 어떤 변화가 나타나기 시작하는 지점.*

그런데 미분은 기본적으로 연속 함수에 대해 정의된 수학 개념이다. 이를 이산 함수에 해당하는 영상에 적용하기 위해서는 미분 함수를 근사화하여 사용해야 한다.

- 순방향 차이 (Forward difference)

$$
\frac{\partial I}{\partial x}\approx \frac{I(x+h,y)-I(x,y)}{h}
$$

- 역방향 차이 (Backward difference)

  $$
  \frac{\partial I}{\partial x}\approx \frac{I(x, y)-I(x-h,y)}{h}
  $$

- 중간값 차이 (Centered difference)

  $$
  \frac{\partial I}{\partial x}\approx \frac{I(x+h,y)-I(x-h,y)}{2h}
  $$

I(x, y)는 영상과 같은 2차원 이산 함수를, h는 이산값의 간격을 의미한다. 미분 근사 방법을 영상에 적용할 경우, h는 픽셀의 간격을 의미하며 1로 간주한다.

중간값 차이를 이용하는 방법이 이론적으로 근사화 오류가 가장 적기 때문에 주로 사용되며, 중간값 차이 방법을 마스크 연산의 형태로 변환하면 그림과 같다. 다만 실제 코드로 구현할 때에는 연산량이 늘어나기 때문에 1/2를 곱하지 않는다.

![image3](https://github.com/ufolozie/ufolozie.github.io/assets/98166928/cea3258f-d247-4b00-8f99-75554de15064){: width="30%" style="display: block; margin: auto;"}

영상은 2차원 공간에서 정의된 함수이기 때문에, 영상에서 미분 값을 이용하여 엣지를 찾기 위해서는 가로 방향과 세로 방향의 미분 값을 함께 사용해야 한다. 2차원 공간상에서의 함수 f(x, y)가 있을 때, 이 함수의 그래디언트(gradient)는 벡터 형태로 정의된다.

$$
\nabla f=\begin{bmatrix}\frac{∂f}{∂x} \\ \frac{∂f}{∂y}\end{bmatrix}
$$

2차원 함수의 그래디언트는 함수 f(x, y)를 x축과 y축으로 각각 편미분하여 벡터 형태로 표현한 것이다. 벡터이기 때문에 그래디언트를 크기와 방향 성분으로 표현할 수 있다. 그래디언트 벡터의 방향이 변화의 정도가 가장 심한 방향이고, 그래디언트 벡터의 크기는 변화의 정도를 나타내는 척도로 생각할 수 있다.

$$
|\nabla f|=\sqrt{ {\left(\frac{∂f}{∂x}\right)}^2 + {\left(\frac{∂f}{∂y}\right)}^2}
$$

그래디언트의 크기를 구하는 수식이다. 영상에서 엣지 위치를 찾기 위해서는 위와 같은 그래디언트의 절댓값을 계산하고, 이 값이 특정 값보다 큰 경우에 한하여 엣지로 정의한다. 여기서 엣지 여부를 판단하기 위해 기준이 되는 값을 임계값이라고 부른다.

위 수식과 중간값 차이에 의한 미분 함수의 근사를 동시에 사용하여 수식을 작성하면 다음과 같은 엣지 검출 조건을 얻을 수 있다.

$$
\sqrt{ {[I(x+1,y)-I(x-1,y)]}^2 + {[I(x,y+1)-I(x,y-1)]}^2}>T
$$

즉, 좌표 (x, y)에서 위 수식을 만족한다면 점 (x, y)는 엣지로 간주할 수 있다. 위 수식에서 T는 임계값을 의미한다.

참고로 구현의 편의를 위하여 그래디언트 크기를 다음과 같이 근사화하기도 한다. 이 경우 곱셈과 제곱근 연산이 없어지므로 구현이 빠르고 간단해진다.

$$
|\nabla f| ≅ \left|\frac{∂f}{∂x}\right| + \left|\frac{∂f}{∂y}\right|
$$

그래디언트 벡터가 가리키는 방향, 즉 가장 경사가 심한 방향은 다음과 같이 삼각함수를 사용하여 구할 수 있다.

$$
\theta = tan^{-1} \left( \frac{∂f}{∂x} \Bigg/ \frac{∂f}{∂y}\right)
$$

<br>

## **마스크 기반 엣지 검출**

![img4](https://github.com/user-attachments/assets/dfd056c6-8827-47d6-99dc-c1571f6fa1f9){: width="70%" style="display: block; margin: auto;"}

로버츠 마스크는 서로 대각선 위치에 있는 픽셀끼리의 차분 형태로, 프리윗과 소벨 마스크는 가로 방향과 세로 방향의 엣지를 구하는 필터의 조합으로 구성되어 있다. 각각의 마스크를 이용하여 엣지를 검출한 결과는 아래와 같다.

![img5](https://github.com/user-attachments/assets/02cb3932-602d-47b8-bade-d4a4cabd9769){: width="70%" style="display: block; margin: auto;"}

위의 그림에서 각 방향에 따라 따로 구해진 엣지 검출 결과를 하나로 합치면 다음과 같은 결과를 얻을 수 있다. 두 방향에 대한 엣지 검출 결과를 합해야 엣지가 선명해짐을 알 수 있다. 이때 그래디언트의 크기를 구하는 수식을 사용하였다.

![img6](https://github.com/user-attachments/assets/54f71695-d713-4b8b-99da-46db92680f79){: width="70%" style="display: block; margin: auto;"}

위 그림에서 나타나는 밝기 차이는 소벨 마스크 필터가 4개의 미분 값을 더한 것(미분을 4번 수행한 것)이고, 프리윗 마스크는 3개의 미분 값을, 로버츠는 오직 하나의 미분 값을 사용한 결과이기 때문이다. 따라서 필연적으로 소벨 마스크 필터가 가장 좋은 성능을 보인다. 아래 그림은 위의 결과에 서로 다른 임계값을 이용하여 이진 영상으로 변환한 결과이다. 밝기 차이가 최종 결과에는 큰 차이를 주지 않음을 확인할 수 있다.

![img7](https://github.com/user-attachments/assets/cabb4e63-4ece-404d-be26-b90393e066f9){: width="70%" style="display: block; margin: auto;"}

###### > 마스크 기반 엣지 검출 구현 함수

```c++
void IppEdgeRoberts(IppByteImage& img, IppByteImage& imgEdge)
{
    int w = img.GetWidth();
    int h = img.GetHeight();

    imgEdge.CreateImage(w, h);

    BYTE** p1 = img.GetPixels2D();
    BYTE** p2 = imgEdge.GetPixels2D();

    int i, j, h1, h2;
    double hval;
    for (j = 1; j < h; j++)
    for (i = 1; i < w - 1; i++)
    {
        h1 = p1[j][i] - p1[j - 1][i - 1];
        h2 = p1[j][i] - p1[j - 1][i + 1];

        hval = sqrt(static_cast<double>(h1 * h1 + h2 * h2));

        p2[j][i] = static_cast<BYTE>(limit(hval + 0.5));
    }
}

void IppEdgePrewitt(IppByteImage& img, IppByteImage& imgEdge)
{
    int w = img.GetWidth();
    int h = img.GetHeight();

    imgEdge.CreateImage(w, h);

    BYTE** p1 = img.GetPixels2D();
    BYTE** p2 = imgEdge.GetPixels2D();

    int i, j, h1, h2;
    double hval;
    for (j = 1; j < h - 1; j++)
    for (i = 1; i < w - 1; i++)
    {
        h1 = -p1[j - 1][i - 1] - p1[j - 1][i] - p1[j - 1][i + 1]
            + p1[j + 1][i - 1] + p1[j + 1][i] + p1[j + 1][i + 1];
        h2 = -p1[j - 1][i - 1] - p1[j][i - 1] - p1[j + 1][i - 1]
            + p1[j - 1][i + 1] + p1[j][i + 1] + p1[j + 1][i + 1];

        hval = sqrt(static_cast<double>(h1 * h1 + h2 * h2));

        p2[j][i] = static_cast<BYTE>(limit(hval + 0.5));
    }
}

void IppEdgeSobel(IppByteImage& img, IppByteImage& imgEdge)
{
    int w = img.GetWidth();
    int h = img.GetHeight();

    imgEdge.CreateImage(w, h);

    BYTE** p1 = img.GetPixels2D();
    BYTE** p2 = imgEdge.GetPixels2D();

    int i, j, h1, h2;
    double hval;
    for (j = 1; j < h - 1; j++)
    for (i = 1; i < w - 1; i++)
    {
        h1 = -p1[j - 1][i - 1] - 2 * p1[j - 1][i] - p1[j - 1][i + 1]
            + p1[j + 1][i - 1] + 2 * p1[j + 1][i] + p1[j + 1][i + 1];
        h2 = -p1[j - 1][i - 1] - 2 * p1[j][i - 1] - p1[j + 1][i - 1]
            + p1[j - 1][i + 1] + 2 * p1[j][i + 1] + p1[j + 1][i + 1];

        hval = sqrt(static_cast<double>(h1 * h1 + h2 * h2));

        p2[j][i] = static_cast<BYTE>(limit(hval + 0.5));
    }
}
```

이들 함수의 첫 번째 인자 img는 입력 영상이며, 두 번째 인자 imgEdge는 엣지 검출 마스크 연산이 수행된 결과가 저장될 영상이다.

<br>

## **캐니 엣지 검출기**

J. Canny의 **좋은 엣지 검출기의 조건을 만족하는 새로운 형태의 엣지 검출 방법**이다.

- **정확한 검출**: 엣지가 아닌 점을 엣지로 찾거나 또는 엣지인 점을 검출하지 못하는 확률을 최소화해야 함
- **정확한 위치**: 실제 엣지의 중심을 찾아야 함
- **단일 엣지**: 하나의 엣지는 하나의 점으로 표현되어야 함

캐니 엣지 검출기는 그래디언트의 크기와 방향을 모두 고려하여 보다 정확하게 엣지 위치를 찾는다. 또한 엣지의 진행 방향에 또 다시 엣지가 나타날 가능성이 높다는 점을 고려하여 그래디언트 크기가 다소 약하게 나타나는 엣지 픽셀도 놓치지 않고 찾을 수 있다.

<br>

### **가우시안 필터링**

가우시안 필터를 이용하여 영상을 부드럽게 만들어 잡음의 영향을 제거해준다. 영상에 있는 백색 잡음의 영향을 제거하기 위해서는 가우시안 필터를 사용하는 것이 가장 효과적이다. 2차원 가우시안 필터의 수식은 다음과 같다.

$$
G_σ(x, y)=\frac{1}{2πσ^2}exp\left(-\frac{x^2+y^2}{2σ^2}\right)
$$

<br>

### **그래디언트 계산 (크기 & 방향)**

캐니 엣지 검출기 역시 그래디언트 크기를 기본으로 엣지 위치를 탐색하며, 그래디언트의 방향까지 함께 고려하여 좀 더 정확한 엣지를 찾는다.

그래디언트 계산은 주로 소벨 마스크를 사용한다. 가로 방향으로의 미분을 계산하는 소벨 마스크 결과 영상을 $G_x$라고 하고 세로 방향으로의 미분을 계산하는 소벨 마스크 결과 영상을 $G_y$라고 할 경우, 그래디언트의 크기와 방향은 다음과 같이 구한다.

$$
|G|=\sqrt{ {G_x}^2+{G_y}^2 }
$$

$$
θ=tan^{-1}\left(\frac{G_y}{G_x}\right)
$$

<br>

### **비최대 억제**

그래디언트 크기가 특정 임계값보다 큰 픽셀들을 엣지 픽셀로 지정할 경우, 하나의 엣지 위치에 대해 여러 개의 픽셀이 동시에 엣지로 판별되는 현상이 발생한다. 비최대 억제는 이와 같은 현상을 없애기 위하여 **그래디언트 크기가 국지적 최대인 픽셀만을 엣지 픽셀로 설정하는 기법**이다.

비최대 억제를 적용하기 위해 그래디언트 방향 성분을 크게 네 개의 구역으로 나눈다. 아래 그림은 0~360° 범위의 각도 성분을 양자화하여 4개의 방향 성분으로 나눈 것이다. 여기서 밝은 색상의 픽셀이 그래디언트 크기가 큰 픽셀, 어두운 픽셀은 상대적으로 그래디언트 크기가 작은 픽셀이다. 흰색 픽셀의 흐름을 나타내는 화살표 방향을 엣지의 방향이라고 하고, 이는 그래디언트 방향과 수직이다. 캐니 엣지 검출기에서 국지적 최대를 검사할 때에는 엣지의 방향에 수직인 방향으로 인접한 픽셀과 크기를 검사한다. 즉, 그림에서 중심 픽셀과 별표 표시된 픽셀들의 그래디언트 크기를 비교하여 국지적 최대인지를 판별한다.

![img8](https://github.com/user-attachments/assets/76f9785e-8988-4b03-9f1e-93a8e9649cb8){: width="85%" style="display: block; margin: auto;"}

<br>

### **이중 임계값을 이용한 히스테리시스 엣지 트래킹**

캐니 엣지 검출기에서는 두 개의 임계값을 사용한다. 두 개의 임계값 중에서 낮은 임계값을 $T_{Low}$라고 표시하고, 높은 임계값을 $T_{High}$라고 표시하자. $T_{High}$보다 큰 그래디언트 값을 갖는 픽셀은 확실하게 엣지 픽셀이라고 인정할 수 있는 픽셀이며, 이를 강한 엣지라고 표현한다. $T_{Low}$보다 작은 그래디언트 값을 갖는 픽셀은 확실하게 엣지 픽셀이 아닌 것으로 간주한다. 그리고 그래디언트 크기 값이 $T_{Low}$보다 크고 $T_{High}$보다 작은 픽셀은 엣지 픽셀이 될 수 있는 픽셀이며, 이를 편의상 약한 엣지라고 표현한다.

캐니 엣지 검출기는 일단 모든 픽셀 중에서 강한 엣지와 약한 엣지를 찾는다. 강한 엣지는 모두 최종적인 엣지 픽셀로 선택되지만, 약한 엣지는 선별하여 일부만 엣지로 판별한다. 이때 사용되는 방법이 히스테리시스 엣지 트래킹 방법이다.

히스테리시스 엣지 트래킹 방법은 엣지 픽셀이 대체로 상호 연결되어 있다는 점을 이용한다. 만약 약한 엣지로 판별된 픽셀이 강한 엣지와 서로 연결되어 있다면, 이 약한 엣지는 최종적으로 엣지 픽셀로 판별한다. 반면에 강한 엣지에 연결되어 있지 않은 약한 엣지는 최종 엣지 픽셀로 선택하지 않는다.

xy 좌표 평면에서 엣지의 흐름을 가로축으로 나타내었고, 그래디언트의 크기를 세로축으로 나타내었다. 세 가지 형태의 엣지 후보에 대하여 최종적으로 엣지로 선택되는 픽셀들은 굵은 실선으로 표현하였다.

![img8](https://github.com/user-attachments/assets/41162917-2ac6-4d51-a54a-98071b23d3b3){: width="75%" style="display: block; margin: auto;"}

###### > 캐니 엣지 검출 구현 함수

```c++
const double PI = 3.14159265358979323846;
const float PI_F = 3.14159265358979323846f;

#define CHECK_WEAK_EDGE(x, y) \
    if (pEdge[y][x] == WEAK_EDGE) { \
        pEdge[y][x] = STRONG_EDGE; \
        strong_edges.push_back(IppPoint(x, y)); \
    }

void IppEdgeCanny(IppByteImage& imgSrc, IppByteImage& imgEdge, float sigma, float th_low, float th_high)
{
    register int i, j;

    int w = imgSrc.GetWidth();
    int h = imgSrc.GetHeight();

    /*
    * 1. 가우시안 필터링
    */

    IppFloatImage imgGaussian(w, h); // float 타입 영상으로 선언
    IppFilterGaussian(imgSrc, imgGaussian, sigma); // 잡음의 영향이 감쇠된 영상

    /*
    * 2. 그래디언트 구하기 (크기 & 방향)
    */

    IppFloatImage imgGx(w, h); // 수평 방향의 소벨 마스크 연산
    IppFloatImage imgGy(w, h); // 수직 방향의 소벨 마스크 연산
    IppFloatImage imgMag(w, h); // 그래디언트 크기

    float** pGauss = imgGaussian.GetPixels2D();
    float** pGx = imgGx.GetPixels2D();
    float** pGy = imgGy.GetPixels2D();
    float** pMag = imgMag.GetPixels2D();

    for (j = 1; j < h - 1; j++)
    for (i = 1; i < w - 1; i++)
    {
        pGx[j][i] = -pGauss[j - 1][i - 1] - 2 * pGauss[j][i - 1] - pGauss[j + 1][i - 1]
            + pGauss[j - 1][i + 1] + 2 * pGauss[j][i + 1] + pGauss[j + 1][i + 1];
        pGy[j][i] = -pGauss[j - 1][i - 1] - 2 * pGauss[j - 1][i] - pGauss[j - 1][i + 1]
            + pGauss[j + 1][i - 1] + 2 * pGauss[j + 1][i] + pGauss[j + 1][i + 1];

        pMag[j][i] = sqrt(pGx[j][i] * pGx[j][i] + pGy[j][i] * pGy[j][i]);
    }

    /*
    * 3. 비최대 억제
    * 국지적 최대를 구함과 동시에 이중 임계값을 적용하여 strong edge와 weak edge를 구한다.
    */

    imgEdge.CreateImage(w, h);
    BYTE** pEdge = imgEdge.GetPixels2D();

    enum DISTRICT { AREA0 = 0, AREA45, AREA90, AREA135, NOAREA };

    const BYTE STRONG_EDGE = 255;
    const BYTE WEAK_EDGE = 128;

    std::vector<IppPoint> strong_edges;

    float ang;
    int district;
    bool local_max;
    for (j = 1; j < h - 1; j++)
    for (i = 1; i < w - 1; i++)
    {
        // 그래디언트 크기가 th_low보다 큰 픽셀에 대해서만 국지적 최대 검사
        // 국지적 최대인 픽셀에 대해서만 강한 엣지 또는 약한 엣지로 설정
        if (pMag[j][i] > th_low)
        {
            // 그래디언트 방향 계산 (4개 구역)
            if (pGx[j][i] != 0.f)
            {
                ang = atan2(pGy[j][i], pGx[j][i]) * 180 / PI_F;
                if (((ang >= -22.5f) && (ang < 22.5f)) || (ang >= 157.5f) || (ang < -157.5f))
                    district = AREA0;
                else if (((ang >= 22.5f) && (ang < 67.5f)) || ((ang >= -157.5f) && (ang <
                    -112.5f)))
                    district = AREA45;
                else if (((ang >= 67.5) && (ang < 112.5)) || ((ang >= -112.5) && (ang < -67.5)))
                    district = AREA90;
                else
                    district = AREA135;
            }
            else
                district = AREA90;

            // 국지적 최대 검사
            local_max = false;
            switch (district)
            {
            case AREA0:
                if ((pMag[j][i] >= pMag[j][i - 1]) && (pMag[j][i] > pMag[j][i + 1]))
                    local_max = true;
                break;
            case AREA45:
                if ((pMag[j][i] >= pMag[j - 1][i - 1]) && (pMag[j][i] > pMag[j + 1][i + 1]))
                    local_max = true;
                break;
            case AREA90:
                if ((pMag[j][i] >= pMag[j - 1][i]) && (pMag[j][i] > pMag[j + 1][i]))
                    local_max = true;
                break;
            case AREA135:
            default:
                if ((pMag[j][i] >= pMag[j - 1][i + 1]) && (pMag[j][i] > pMag[j + 1][i - 1]))
                    local_max = true;
                break;
            }

            // 강한 엣지와 약한 엣지 구분
            if (local_max)
            {
                if (pMag[j][i] > th_high)
                {
                    pEdge[j][i] = STRONG_EDGE;
                    strong_edges.push_back(IppPoint(i, j));
                }
                else
                    pEdge[j][i] = WEAK_EDGE;
            }
        }
    }

    /*
    * 4. 히스테리시스 엣지 트래킹
    */

    while (!strong_edges.empty())
    {
        IppPoint p = strong_edges.back();
        strong_edges.pop_back();

        int x = p.x, y = p.y;

        // 강한 엣지 주변의 약한 엣지는 최종 엣지(강한 엣지)로 설정
        CHECK_WEAK_EDGE(x + 1, y     )
        CHECK_WEAK_EDGE(x + 1, y + 1)
        CHECK_WEAK_EDGE(x      , y + 1)
        CHECK_WEAK_EDGE(x - 1, y + 1)
        CHECK_WEAK_EDGE(x - 1, y     )
        CHECK_WEAK_EDGE(x - 1, y - 1)
        CHECK_WEAK_EDGE(x     , y - 1)
        CHECK_WEAK_EDGE(x + 1, y - 1)
    }

    for (j = 0; j < h; j++)
    for (i = 0; i < w; i++)
    {
        // 끝까지 약한 엣지로 남아있는 픽셀은 모두 엣지가 아닌 것으로 판단
        if (pEdge[j][i] == WEAK_EDGE) pEdge[j][i] = 0;
    }
}
```

<br>

## **허프 변환을 이용한 직선 검출**

허프 변환은 **2차원 영상 좌표에서 직선의 방정식을 파라미터 공간으로 변환하여 직선을 찾는 알고리즘**이다.

일반적으로 2차원 xy 좌표 공간에서의 직선의 방정식은 y = ax + b 와 같이 나타낼 수 있으며, a는 기울기이고 b는 y 절편이다. 이때 a와 b는 직선의 모양을 결정하는 파라미터라고 할 수 있다.

이 방정식을 가로축이 a이고 세로축이 b인 ab 공간에서 정의한다면 식은 b = -xa + y 처럼 쓸 수 있다.

위와 같이 xy 공간에서의 직선의 방정식을 ab 공간으로 가져오면 직선이었던 것이 한 점으로 나타나게 되고, 반대로 xy 공간에서 점이었던 것은 ab 공간에서 직선의 형태로 나타나게 된다.

![img10](https://github.com/user-attachments/assets/e471a450-a415-4195-97e9-9ce68ba01f58){: width="70%" style="display: block; margin: auto;"}

이러한 원리로 위의 그림에서 두 직선이 서로 교차하는 점의 좌표는 (a’, b’)가 되며, 이는 xy 공간에서의 직선의 방정식 y = a’x + b’ 를 정의해주는 두 개의 파라미터 값이다.

허프 변환을 이용해 직선의 방정식을 찾기 위해서는 xy 공간에서 엣지로 판명된 모든 점의 좌표를 ab 파라미터 공간으로 변경하여 직선을 그려주고, 직선들이 많이 교차되는 점을 찾으면 된다. 이때 직선이 교차하는 점을 찾기 위해 일반적으로 축적 배열을 사용한다.

**축적 배열**이란, **일반적인 2차원 배열을 만든 후 직선이 지나가는 위치의 배열 원소의 값을 1씩 증가시키는 배열**이다.

![img11](https://github.com/user-attachments/assets/1ad44334-5f95-4828-a53e-df9b66102ea3){: width="70%" style="display: block; margin: auto;"}

1. 왼쪽의 2차원 xy 영상 좌표계에 있는 3개의 점을 오른쪽의 ab 공간으로 가져와 3개의 직선으로 바꾼다.
2. 직선이 지나가는 배열 좌표의 값을 1씩 증가시킨다.
3. 3개의 직선이 공통으로 지나간 위치의 값이 존재하게 될 것이며, 최종적으로 최댓값이 존재하는 위치를 찾아 그 위치의 배열의 인덱스에 해당하는 a와 b를 찾으면 원래 직선의 방정식의 파라미터 값(a와 b)을 구할 수 있다.

즉, ab 공간에서 가장 많이 축적된 부분을 사용하게 되는 것이다.

그러나 위와 같은 직선의 방정식을 이용하는 방법은 y축과 평행한 직선을 표현하지 못한다는 한계가 있다. 수직선의 경우 y = ax + b에서 기울기 a의 값이 무한대이기 때문이다.

따라서 실제 허프 변환을 구현할 때는 xsinθ + ycosθ = ρ 와 같은 극좌표계 형태를 사용한다.

![img12](https://github.com/user-attachments/assets/a8870841-88d9-4a23-b616-37a4d4f1b0be){: width="35%" style="display: block; margin: auto;"}

ρ는 원점에서 직선까지의 수직 거리를, θ는 원점에서 직선에 수직선을 그렸을 때 y축과 이루는 각도 크기를 의미한다.

이때, ρ와 θ가 가질 수 있는 값의 범위에는 제한이 있다.

$$
-\sqrt{M^2+N^2} ≤ ρ ≤ \sqrt{M^2+N^2}
$$

$$
0 ≤ \theta ≤ \pi
$$

M과 N은 각각 영상의 가로와 세로 픽셀의 크기를 의미한다.

![img13](https://github.com/user-attachments/assets/aa1e9caa-b14d-4f56-9a0e-453a9de6a429){: width="50%" style="display: block; margin: auto;"}

영상의 xy 좌표 공간에서 실제 직선의 모양에 따른 ρ와 θ의 값의 범위는 다음과 같이 나타낼 수 있다. x와 y가 모두 음수인 직선은 실제 영상에서는 존재할 수 없으므로 회색으로 표현하였다.

이처럼 ρθ 공간을 사용해 허프 변환을 구현할 때도 축적 배열을 사용한다. 다만 극좌표계로 변경하는 것에 유의해야 한다.

###### > 허프 변환 결과

![img14](https://github.com/user-attachments/assets/7a281b26-f9ba-4b11-a543-eeb2ecf69a8b){: width="70%" style="display: block; margin: auto;"}

위에서부터 차례로 입력 영상, 축적 배열 영상, 직선 검출 결과.

입력 영상에서는 오직 점 하나를 가지고 있는 영상에 허프 변환을 적용한다. 축적 배열은 오직 하나의 커브가 나타나게 되며, 오직 하나의 점만이 존재하기 때문에 수많은 직선의 방정식이 나올 수 있어 직선을 특정하지 못하고 임의의 직선을 표현한다.

축적 배열 영상에는 두 개의 점이 있다. 축적 배열 모습을 보면 두 개의 커브가 존재하며, 이 커브들이 만나는 교차점에서 가장 큰 배열 값을 가질 것이라 예상할 수 있다. 따라서 이 위치에서의 ρ와 θ 값을 이용해 직선을 그려주면 원본 영상의 직선을 검출할 수 있다.

직선 검출 결과는 세 개의 점을 가지고 있다. 축적 배열에서 세 개의 커브와 세 개의 교차점이 발생하게 된다. B 영상과 마찬가지로 세 교차점에 해당하는 위치에서의 ρ와 θ 값을 이용해 직선을 그리면 삼각형 형태의 직선을 검출할 수 있다.

만약 직선을 많이 검출하고 싶다면 임계값을 낮추면 되고, 큼직한 직선들만을 검출하고 싶다면 임계값을 높이면 된다.

---

🗂️ **룩업 테이블**은 프로그램 실행 중 중복되는 연산의 값을 미리 계산하여 저장해두는 배열을 말한다.  
허프 변환을 실제로 구현할 때, ρ 값을 결정하기 위해 모든 각도에 대하여 xsinθ + ycosθ 연산을 수행해야 한다. 이때 삼각함수의 연산을 상당 횟수 중복하여 사용하게 되기 때문에 이를 미리 계산하여 따로 룩업 테이블에 저장해두고 사용하는 것이 효율적이다.

---

###### > 허프 변환을 이용한 직선 검출 함수

```c
void IppHoughLine(IppByteImage& img, std::vector<IppLineParam>& lines, int threshold)
{
    register int i, j;

    int w = img.GetWidth();
    int h = img.GetHeight();

    BYTE** ptr = img.GetPixels2D();
    int num_rho = static_cast<int>(sqrt((double)w*w + h*h) * 2);
    int num_ang = 360;

    /*
    * 0 ~ PI 각도에 해당하는 sin, cos 함수의 값을 룩업테이블에 저장
    */

    float* sin_tbl = new float[num_ang];
    float* cos_tbl = new float[num_ang];

    for (i = 0; i < num_ang; i++)
    {
        sin_tbl[i] = sin(i * PI_F / num_ang);
        cos_tbl[i] = cos(i * PI_F / num_ang);
    }

    /*
    * 축적 배열(Accumulate array) 생성
    */

    IppIntImage imgAcc(num_ang, num_rho);
    int** pAcc = imgAcc.GetPixels2D();

    int m, n;
    for (j = 0; j < h; j++)
    for (i = 0; i < w; i++)
    {
        if (ptr[j][i] > 128)
        {
            for (n = 0; n < num_ang; n++)
            {
                m = static_cast<int>(floor(i * sin_tbl[n] + j * cos_tbl[n] + 0.5f));
                m += (num_rho / 2);

                pAcc[m][n]++;
            }
        }
    }

    /*
    * 임계값보다 큰 국지적 최댓값최댓값을 찾아 직선 성분으로 결정
    */

    lines.clear();
    int temp;
    for (m = 0; m < num_rho; m++)
    for (n = 0; n < num_ang; n++)

    {
        temp = pAcc[m][n];
        if (temp > threshold)
        {
            if (temp > pAcc[m - 1][n] && temp > pAcc[m - 1][n + 1] &&
                temp > pAcc[m][n + 1] && temp > pAcc[m + 1][n + 1] &&
                temp > pAcc[m + 1][n] && temp > pAcc[m + 1][n - 1] &&
                temp > pAcc[m][n - 1] && temp > pAcc[m - 1][n - 1])
            {
                lines.push_back(IppLineParam(m - (num_rho / 2), n * PI / num_ang, pAcc[m][n]));
            }
        }
    }

    /*
    * 동적 할당한 메모리 해제
    */

    delete[] sin_tbl;
    delete[] cos_tbl;
}
```

<br>

## **해리스 코너 포인트 검출**

해리스 코너 검출 방법은 **영상의 코너 포인트(모서리 부분)를 찾는 기본 알고리즘**이다.

일반적으로 코너 포인트는 사물과 사물 혹은 배경 사이에 위치하므로 영상에서 사물의 위치 및 형태를 가늠할 수 있는 기반이 된다.

해리스 코너 검출 방법은 작은 윈도우를 상하좌우로 움직이며 윈도우 안의 픽셀 값의 변화를 분석하여 코너 여부를 판별한다. 윈도우 안의 픽셀 값이 상하좌우 방향으로 모두 급격하게 변하는 위치를 코너로 규정하는 것이다.

![img15](https://github.com/user-attachments/assets/33b12e73-bf55-4d3a-8fba-fd5006824e64){: width="75%" style="display: block; margin: auto;"}

상하좌우, 각 방위의 대각선으로 총 8 방향에 대해 타겟 픽셀에서 좌표를 이동시켰을 때의 변화량을 계산했을 때, 이 변화량이 지역적으로 극대가 되는 지점이 코너 포인트(국지적 최대값)가 된다.

주변 픽셀 값과의 차이, 다시 말해 변화량을 보는 수식은 아래와 같이 정의한다.

$$
c(x, y) = \sum_{W}[I(x, y)-I(x+Δx, y+Δy)]^2
$$

테일러 급수에 의해 위의 변화량 수식을 근사화하면 다음과 같이 전개할 수 있다.

$$
c(x, y) = \sum_{W}\Bigg(\begin{bmatrix}I_{x}(x, y) & I_{y}(x, y)\end{bmatrix}
\begin{bmatrix}Δx \\ Δy\end{bmatrix}\Bigg)^2
$$

$$
= \begin{bmatrix}Δx & Δy\end{bmatrix}
\begin{bmatrix}\sum_W(I_x(x, y))^2 & \sum_W(I_x(x, y)I_y(x, y)) \\
\sum_W(I_x(x, y)I_y(x, y)) & \sum_W(I_y(x, y))^2\end{bmatrix}
\begin{bmatrix}Δx \\ Δy\end{bmatrix}
$$

이때 중앙의 2×2 행렬을 M이라고 하고 이 행렬 M의 고유값을 $λ_1$과 $λ_2$라고 표현한다면, $λ_1$과 $λ_2$의 값의 크기에 따라 타겟 픽셀이 평면인지 엣지인지 혹은 코너 포인트인지 결정할 수 있다. 즉 행렬 M은 함수 c(x, y)의 모양을 결정하는 행렬인 것이다.  
**고유값: A가 n×n 정방행렬일 때, 적당한 스칼라 λ와 0이 아닌 벡터 v가 존재해서 Av = λv 의 관계를 만족할 때의 λ. n차 정방행렬은 n개의 고유값을 가짐.*

만약 $λ_1$과 $λ_2$ 모두 작은 값을 가지면 평면에 속한 점으로 볼 수 있고, 둘 중 하나만 큰 값을 가지면 엣지에 위치한 픽셀이다. 만약 $λ_1$과 $λ_2$ 모두 큰 값을 가지면 이는 코너 포인트에 위치한 픽셀일 가능성이 크다.

그러나 해리스 코너 포인트 검출 방법에서는 행렬 M의 고유값을 직접 구하는 대신 코너 응답 함수를 새롭게 정의하여 사용한다. 코너 응답 함수 R은 R = Det(M) - k * Tr(M)$^2$ 와 같이 정의하며, Det(M)은 행렬 M의 행렬식, Tr(M)은 대각합을 의미한다. 2차 정방행렬 M에서 행렬식과 대각합은 고유값 $λ_1$, $λ_2$에 대해 두 가지 성질을 만족한다.

$$
Det(M) = λ_1·λ_2
$$

$$
Tr(M) = λ_1+λ_2
$$

즉, 행렬식과 대각합을 이용하여 고유값의 곱과 합을 구해 사용하겠다는 것이다.

일반적으로 상수 k의 값으로는 0.04 ~ 0.06 사이의 값을 부여한다.

코너 응답 함수 R의 값에 따라 평탄한 영역, 엣지 픽셀, 코너 픽셀을 구분하면 다음과 같이 구간을 분리할 수 있다.

![img16](https://github.com/user-attachments/assets/60a30862-0b87-4a80-b404-0db363cfd391){: width="55%" style="display: block; margin: auto;"}

해리스 코너 검출 방법을 이용하면 회전과 어느 정도의 크기 변환에 구애받지 않고 정확하게 코너 포인트를 찾아낼 수 있어 다양한 영상 처리 분야에서 많이 사용된다.

###### > 해리스 코너 검출 구현 함수

```c
void IppHarrisCorner(IppByteImage& img, std::vector<IppPoint>& corners, double th)
{
    register int i, j, x, y;

    int w = img.GetWidth();
    int h = img.GetHeight();

    BYTE** ptr = img.GetPixels2D();

    /*
    * 1. (fx)*(fx), (fx)*(fy), (fy)*(fy) 계산
    */

    IppFloatImage imgDx2(w, h);
    IppFloatImage imgDy2(w, h);
    IppFloatImage imgDxy(w, h);

    float** dx2 = imgDx2.GetPixels2D();
    float** dy2 = imgDy2.GetPixels2D();
    float** dxy = imgDxy.GetPixels2D();

    float tx, ty;
    for (j = 1; j < h - 1; j++)
    for (i = 1; i < w - 1; i++)
    {
        tx = (ptr[j - 1][i + 1] + ptr[j][i + 1] + ptr[j + 1][i + 1]
            - ptr[j - 1][i - 1] - ptr[j][i - 1] - ptr[j + 1][i - 1]) / 6.f;
        ty = (ptr[j + 1][i - 1] + ptr[j + 1][i] + ptr[j + 1][i + 1]
            - ptr[j - 1][i - 1] - ptr[j - 1][i] - ptr[j - 1][i + 1]) / 6.f;

        dx2[j][i] = tx * tx;
        dy2[j][i] = ty * ty;
        dxy[j][i] = tx * ty;
    }

    /*
    * 2. 가우시안 필터링
    */

    IppFloatImage imgGdx2(w, h);
    IppFloatImage imgGdy2(w, h);
    IppFloatImage imgGdxy(w, h);

    float** gdx2 = imgGdx2.GetPixels2D();
    float** gdy2 = imgGdy2.GetPixels2D();
    float** gdxy = imgGdxy.GetPixels2D();

    float g[5][5] = { { 1, 4, 6, 4, 1 },{ 4, 16, 24, 16, 4 },
    { 6, 24, 36, 24, 6 },{ 4, 16, 24, 16, 4 },{ 1, 4, 6, 4, 1 } };

    for (y = 0; y < 5; y++)
    for (x = 0; x < 5; x++)
    {
        g[y][x] /= 256.f;
    }

    float tx2, ty2, txy;
    for (j = 2; j < h - 2; j++)
    for (i = 2; i < w - 2; i++)
    {
        tx2 = ty2 = txy = 0;
        for (y = 0; y < 5; y++)
        for (x = 0; x < 5; x++)
        {
            tx2 += (dx2[j + y - 2][i + x - 2] * g[y][x]);
            ty2 += (dy2[j + y - 2][i + x - 2] * g[y][x]);
            txy += (dxy[j + y - 2][i + x - 2] * g[y][x]);
        }

        gdx2[j][i] = tx2;
        gdy2[j][i] = ty2;
        gdxy[j][i] = txy;
    }

    /*
    * 3. 코너 응답 함수 생성
    */

    IppFloatImage imgCrf(w, h);
    float** crf = imgCrf.GetPixels2D();

    float k = 0.04f;
    for (j = 2; j < h - 2; j++)
    for (i = 2; i < w - 2; i++)
    {
        crf[j][i] = (gdx2[j][i] * gdy2[j][i] - gdxy[j][i] * gdxy[j][i])
            - k*(gdx2[j][i] + gdy2[j][i])*(gdx2[j][i] + gdy2[j][i]);
    }

    /*
    * 4. 임계값보다 큰 국지적 최댓값을 찾아 코너 포인트로 결정
    */

    corners.clear();
    float cvf_value;

    for (j = 2; j < h - 2; j++)
    for (i = 2; i < w - 2; i++)
    {
        cvf_value = crf[j][i];
        if (cvf_value > th)
        {
            if (cvf_value > crf[j - 1][i] && cvf_value > crf[j - 1][i + 1] &&
                cvf_value > crf[j][i + 1] && cvf_value > crf[j + 1][i + 1] &&
                cvf_value > crf[j + 1][i] && cvf_value > crf[j + 1][i - 1] &&
                cvf_value > crf[j][i - 1] && cvf_value > crf[j - 1][i - 1])
            {
                corners.push_back(IppPoint(i, j));
            }
        }
    }
}
```

<br>
